# Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Distribution abstract base class."""

import abc
import collections.abc
import contextlib
import functools
import operator
import typing

from typing import Sequence, Tuple, Union

import chex
from distrax._src.utils import jittable
import jax
import jax.numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp


tfd = tfp.distributions

Array = chex.Array
PRNGKey = chex.PRNGKey
IntLike = Union[int, np.int16, np.int32, np.int64]


class Distribution(jittable.Jittable, metaclass=abc.ABCMeta):
  """Jittable abstract base class for all Distrax distributions."""

  @abc.abstractmethod
  def _sample_n(self, key: PRNGKey, n: int) -> Array:
    """Returns `n` samples."""

  def _sample_n_and_log_prob(self, key: PRNGKey, n: int) -> Tuple[Array, Array]:
    """Returns `n` samples and their log probs.

    By default, it just calls `log_prob` on the generated samples. However, for
    many distributions it's more efficient to compute the log prob of samples
    than of arbitrary events (for example, there's no need to check that a
    sample is within the distribution's domain). If that's the case, a subclass
    may override this method with a more efficient implementation.

    Args:
      key: PRNG key.
      n: Number of samples to generate.

    Returns:
      A tuple of `n` samples and their log probs.
    """
    samples = self._sample_n(key, n)
    log_prob = self.log_prob(samples)
    return samples, log_prob

  @abc.abstractmethod
  def log_prob(self, value: Array) -> Array:
    """Calculates the log probability of an event.

    Args:
      value: An event.

    Returns:
      The log probability log P(value).
    """

  def prob(self, value: Array) -> Array:
    """Calculates the probability of an event.

    Args:
      value: An event.

    Returns:
      The probability P(value).
    """
    return jnp.exp(self.log_prob(value))

  @property
  @abc.abstractmethod
  def event_shape(self) -> Tuple[int, ...]:
    """Shape of event of distribution samples."""

  @property
  def batch_shape(self) -> Tuple[int, ...]:
    """Shape of batch of distribution samples."""
    sample_shape = jax.eval_shape(
        lambda: self.sample(seed=jax.random.PRNGKey(0), sample_shape=())).shape
    if not self.event_shape:
      return sample_shape
    return sample_shape[:-len(self.event_shape)]

  @property
  def name(self) -> str:
    """Distribution name."""
    return type(self).__name__

  @property
  def dtype(self) -> jnp.dtype:
    """The data type of the samples generated by the distribution."""
    return jax.eval_shape(
        lambda: self.sample(seed=jax.random.PRNGKey(0), sample_shape=())).dtype

  def sample(self,
             *,
             seed: Union[IntLike, PRNGKey],
             sample_shape: Union[IntLike, Sequence[IntLike]] = ()) -> Array:
    """Samples an event.

    Args:
      seed: PRNG key or integer seed.
      sample_shape: Additional leading dimensions for sample.

    Returns:
      A sample of shape `sample_shape` + `batch_shape` + `event_shape`.
    """
    rng, sample_shape = convert_seed_and_sample_shape(seed, sample_shape)
    num_samples = functools.reduce(operator.mul, sample_shape, 1)  # product

    samples = self._sample_n(rng, num_samples)

    return samples.reshape(sample_shape + samples.shape[1:])

  def sample_and_log_prob(
      self,
      *,
      seed: Union[IntLike, PRNGKey],
      sample_shape: Union[IntLike, Sequence[IntLike]] = ()
  ) -> Tuple[Array, Array]:
    """Returns a sample and associated log probability. See `sample`."""
    rng, sample_shape = convert_seed_and_sample_shape(seed, sample_shape)
    num_samples = functools.reduce(operator.mul, sample_shape, 1)  # product

    samples, log_prob = self._sample_n_and_log_prob(rng, num_samples)

    samples = samples.reshape(sample_shape + samples.shape[1:])
    log_prob = log_prob.reshape(sample_shape + log_prob.shape[1:])
    return samples, log_prob

  def kl_divergence(self, other_dist, **kwargs) -> Array:
    """Calculates the KL divergence to another distribution.

    Args:
      other_dist: A compatible Distax or TFP Distribution.
      **kwargs: Additional kwargs.

    Returns:
      The KL divergence `KL(self || other_dist)`.
    """
    return tfd.kullback_leibler.kl_divergence(self, other_dist, **kwargs)

  def entropy(self) -> Array:
    """Calculates the Shannon entropy (in nats)."""
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `entropy`.')

  def log_cdf(self, value: Array) -> Array:
    """Evaluates the log cumulative distribution function at `value`.

    Args:
      value: An event.

    Returns:
      The log CDF evaluated at value, i.e. log P[X <= value].
    """
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `log_cdf`.')

  def cdf(self, value: Array) -> Array:
    """Evaluates the cumulative distribution function at `value`.

    Args:
      value: An event.

    Returns:
      The CDF evaluated at value, i.e. P[X <= value].
    """
    return jnp.exp(self.log_cdf(value))

  def mean(self) -> Array:
    """Calculates the mean."""
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `mean`.')

  def median(self) -> Array:
    """Calculates the median."""
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `median`.')

  def variance(self) -> Array:
    """Calculates the variance."""
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `variance`.')

  def stddev(self) -> Array:
    """Calculates the standard deviation."""
    return jnp.sqrt(self.variance())

  def mode(self) -> Array:
    """Calculates the mode."""
    raise NotImplementedError(
        f'Distribution `{self.name}` does not implement `mode`.')

  def cross_entropy(self, other_dist, **kwargs) -> Array:
    """Calculates the cross entropy to another distribution.

    Args:
      other_dist: A compatible Distax or TFP Distribution.
      **kwargs: Additional kwargs.

    Returns:
      The cross entropy `H(self || other_dist)`.
    """
    return self.kl_divergence(other_dist, **kwargs) + self.entropy()

  @contextlib.contextmanager
  def _name_and_control_scope(self, *unused_a, **unused_k):
    yield


def convert_seed_and_sample_shape(
    seed: Union[IntLike, PRNGKey],
    sample_shape: Union[IntLike, Sequence[IntLike]]
) -> Tuple[PRNGKey, Tuple[int, ...]]:
  """Shared functionality to ensure that seeds and shapes are the right type."""

  if not isinstance(sample_shape, collections.abc.Sequence):
    sample_shape = (sample_shape,)
  sample_shape = tuple(map(int, sample_shape))

  if isinstance(seed, IntLike.__args__):
    rng = jax.random.PRNGKey(seed)
  else:  # key is of type PRNGKey
    rng = seed

  return rng, sample_shape

DistributionLike = Union[Distribution, tfd.Distribution]
DistributionT = typing.TypeVar('DistributionT', bound=Distribution)
